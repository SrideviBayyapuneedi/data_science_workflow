{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Frame-the-Problem\" data-toc-modified-id=\"Frame-the-Problem-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Frame the Problem</a></span></li><li><span><a href=\"#Select-a-performance-measure\" data-toc-modified-id=\"Select-a-performance-measure-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Select a performance measure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regression-problems\" data-toc-modified-id=\"Regression-problems-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Regression problems</a></span></li><li><span><a href=\"#Classification-Problems\" data-toc-modified-id=\"Classification-Problems-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Classification Problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Confusion Matrix</a></span></li><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Accuracy</a></span></li><li><span><a href=\"#False-Negatives-and-False-Positives\" data-toc-modified-id=\"False-Negatives-and-False-Positives-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>False Negatives and False Positives</a></span></li><li><span><a href=\"#Precision\" data-toc-modified-id=\"Precision-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Precision</a></span></li><li><span><a href=\"#Recall\" data-toc-modified-id=\"Recall-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Recall</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>F1 Score</a></span></li><li><span><a href=\"#F-Beta-Score\" data-toc-modified-id=\"F-Beta-Score-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>F-Beta Score</a></span></li><li><span><a href=\"#Receiver-Operating-Characteristics\" data-toc-modified-id=\"Receiver-Operating-Characteristics-2.2.8\"><span class=\"toc-item-num\">2.2.8&nbsp;&nbsp;</span>Receiver Operating Characteristics</a></span></li></ul></li></ul></li><li><span><a href=\"#Check-Assumptions\" data-toc-modified-id=\"Check-Assumptions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Check Assumptions</a></span></li><li><span><a href=\"#Create-the-workspace-and-an-isolated-environment\" data-toc-modified-id=\"Create-the-workspace-and-an-isolated-environment-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create the workspace and an isolated environment</a></span></li><li><span><a href=\"#Download-the-data:\" data-toc-modified-id=\"Download-the-data:-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Download the data:</a></span></li><li><span><a href=\"#Take-a-Quick-look-at-the-data-structure\" data-toc-modified-id=\"Take-a-Quick-look-at-the-data-structure-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Take a Quick look at the data structure</a></span></li><li><span><a href=\"#Create-a-test-set-and-training-set\" data-toc-modified-id=\"Create-a-test-set-and-training-set-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Create a test set and training set</a></span></li><li><span><a href=\"#Looking-for-correlations\" data-toc-modified-id=\"Looking-for-correlations-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Looking for correlations</a></span></li><li><span><a href=\"#Experiement-with-attribute-combinations\" data-toc-modified-id=\"Experiement-with-attribute-combinations-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Experiement with attribute combinations</a></span></li><li><span><a href=\"#Data-transformation\" data-toc-modified-id=\"Data-transformation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Data transformation</a></span></li><li><span><a href=\"#Cleaning-data---Fill-missing-values\" data-toc-modified-id=\"Cleaning-data---Fill-missing-values-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Cleaning data - Fill missing values</a></span></li><li><span><a href=\"#Handling-text-and-categorical-data\" data-toc-modified-id=\"Handling-text-and-categorical-data-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Handling text and categorical data</a></span></li><li><span><a href=\"#Building-a-custom-class-to-do-custom-data-transformations\" data-toc-modified-id=\"Building-a-custom-class-to-do-custom-data-transformations-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Building a custom class to do custom data transformations</a></span></li><li><span><a href=\"#Feature-Scaling---very-important\" data-toc-modified-id=\"Feature-Scaling---very-important-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Feature Scaling - very important</a></span></li><li><span><a href=\"#Creating-a-Pipeline\" data-toc-modified-id=\"Creating-a-Pipeline-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Creating a Pipeline</a></span></li><li><span><a href=\"#CROSS-VALIDATION\" data-toc-modified-id=\"CROSS-VALIDATION-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>CROSS VALIDATION</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-complexity-graph\" data-toc-modified-id=\"Model-complexity-graph-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;</span>Model complexity graph</a></span></li></ul></li><li><span><a href=\"#Learning-Curve\" data-toc-modified-id=\"Learning-Curve-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Learning Curve</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Look at the Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Frame the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    a. Understand the business objective\n",
    "        What is the business objective? How does the company expect to use this model and benefit from this model?\n",
    "    b. Frame the problem into\n",
    "        i. Supervised Learning\n",
    "        ii. Unsupervised Learing\n",
    "        iii. Reinforcement Learning\n",
    "        iv. Batch learning or Online Learning\n",
    "        v. Instance based learning or model based learning\n",
    "    c. Is there a current model and how does it perform?\n",
    "    d. What is the process right now and why are we building the model only now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Select a performance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. RMSE\n",
    "2. MAE - Mean Absolute Error\n",
    "    - This method is not differentiable, thus is not suitable in places where you want to use gradient decent algorithms, alternative to this, we can use Mean Squared Error\n",
    "3. Mean Squared Error\n",
    "4. R2-score\n",
    "    - Build a simple/baseline model M1, such as mean\n",
    "    - Build a new model M2, to improve your prediction\n",
    "    - R2-score = 1 - (MSE of M2/MSE of M1)\n",
    "    - R2 score lies between 1 and 0. A score closer to 1 is better.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Confusion Matrix gives details about how the data instances were classified. Different metric are calculated using this matrix.\n",
    "\n",
    "1. True Positive: Number of instances that belong to the positive class and have been correctly classified as positive by the model\n",
    "2. True Negative: Number of instances that belong to the negative class and have been correctly classified as negative by the model\n",
    "3. False Negative: Number of instances that belong to the positive class but have been **incorrectly classified as negative by the model**\n",
    "4. False Positive: Number of instances that belong to the negative class but have been **incorrectly classified as positive by the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Type 1 and Type 2 Errors\n",
    "Sometimes in the literature, you'll see False Positives and False Negatives as Type 1 and Type 2 errors. Here is the correspondence:\n",
    "\n",
    "Type 1 Error (Error of the first kind, or False Positive): In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "Type 2 Error (Error of the second kind, or False Negative): In the medical example, this is when we misdiagnose a sick patient as healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Accuracy measures number of points that were correctly classified. Form the above picture of the confusion matrix, we can see that the number of points correctly classfied is the sum of instances that where True Positive and True Negtaive.\n",
    "\n",
    "Accuracy = No.of instances truly classified/ Total number of instances\n",
    "\n",
    "Pros:\n",
    "1. Simple measure to calculate\n",
    "2. Tells the overall performance of the model\n",
    "\n",
    "Cons:\n",
    "1. Does not work well when the classes are imbalanced\n",
    "2. Does not capture the objective of classification when one classes is of more importance than the other\n",
    "In these scenarios we use the metrics defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### False Negatives and False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Depending on the goal of the project, our objective can be to reduce the number or False Negatives or to reduce the number of False Positives.\n",
    "\n",
    "When classifiying spam and non-spam emails, reducing the number of False Positives is more important. Similarly, when trying to diagnose a disease, it is more important to reduce the number of False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Precision: Out of the instances classified as Positive by the model, how many are actually positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Recall: Out of the instances that are actually positive, how many were classified as positive by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "F1 score is a way of combining precision and recall, it is the harmonic mean of precision and recall\n",
    "<img src=\"f1_score_formula.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"am_hm_formula.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### F-Beta Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "F score gives equal importance to precision and recall. But when we have a use case like spam email detection, where precision is more important we would like to give precision more importance. In such cases we calculate F-beta score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"f_beta_formula.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Receiver Operating Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Steps to build a ROC curve:\n",
    "1. For each theshold for splitting the data as positive class and negative class\n",
    "2. Calculate the True Positive Rate and False Positive Rate\n",
    "   -  True Positive Rate = Number of instances classified as positive by the model / Number of positivly labelled instances\n",
    "   -  False Positive Rate = False Positive / Number of negatively labelled instances\n",
    "   \n",
    "3. Now plot the ROC curve with TPR on the y-axis and FPR on the x-axis, **the area under the curve, AUC is the metric calculated here **\n",
    "4. The Area Under the Curve tells us how are model is performing when compared to a random model. For a random model, the AUC is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do you choose the cut-off value of probability to decide which instance belongs to which class?\n",
    "http://ethen8181.github.io/machine-learning/unbalanced/unbalanced.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Check Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check with different teams involved in the project if you have captured the points correctly and you are not making any assumptionsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create the workspace and an isolated environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Download the data: \n",
    "1. Automate downloading the data, write a function\n",
    "2. If necessary write a function to read the data and set dtype of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Take a Quick look at the data structure\n",
    "Use the following commands\n",
    "1. df.head()\n",
    "2. df.info() to know the number of rows, columns and non-null values and datatypes of each column\n",
    "3. If you observe that there are categorical data columns, investigate to find the distribution of different categories\n",
    "4. For numerical columns, use the df.describe() to understand the centrality and deviation\n",
    "5. Plot a histogram of the numerical columns using df.hist(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create a test set and training set\n",
    "Create a test set and keep it aside and never look at it, to avoid data snooping error.\n",
    "1. If the dataset is fixed then you can use a non-hashing technique to split the test and train datasets\n",
    "2. If there is a chance that the dataset can be updated/refreshed from time to time then use a hashing based test and train set.\n",
    "3. The test set/validation set is usually 20% of all the training data\n",
    "\n",
    "Shuffling is of two types, random shuffling and stratified shuffling split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Discover and Visualize the data using the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Things to keep in mind before we start exploring the data.\n",
    "1. Data exploration should be done on the training set only!!\n",
    "2. If the training set is large enough, sample the data to make exploration fast and easy\n",
    "3. Make sure exploration does not change the data in the training set, to avoid this, exploration could be done on a copy of the training dataset\n",
    "\n",
    "Play around with the visualization parameters to make patterns stand out\n",
    "\n",
    "This step should be thorogh and is iterative with all other steps below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Looking for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Depending on the size of the dataset, calculate the correlation coefficient between every pair of attributes, especially between the dependent variable and the independent variable. Keep in mind that correlation only measures the linear relationship between the variables and would completly miss out the non-linear relationship. Plot the scatter plot for  the pair of variables to see the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Experiement with attribute combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using domain knowledge, insights from the data visualization, create new interesting features to be used in the ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for ML Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. During the data exploration phase, if you find attributes with heavy tails, you might want to transform the data by computing their logirthm\n",
    "2. Clean any outliers, capped instances\n",
    "\n",
    "** This step needs to be done in the data exploration phase as well **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cleaning data - Fill missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most of the ML algorithms do not work wel when they have missing values. Below are few methods to fill missing values.\n",
    "1. Fill them with zero, median or mean. \n",
    "    - When using this technique make sure that you are storing the value being used to fill the null values in a variable, this same value needs to e used for filling the test set.\n",
    "    - Also think of strategies to fill missing values in the training data for all the columns even if they do not have any missing values right now\n",
    "\n",
    "2. Delete the rows with missing values, based on the % of missing values for an instance\n",
    "3. Delete an attribute, based on the % of the missing values for that attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Handling text and categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Most ML algorithms work with numbers, we have to convert the text in categorical columns to numbers. \n",
    "- Mapping each category to a number is not the right way to do it since the machine learning algorithms will consider two near by values more similar than 2 distant values, which is not the case.\n",
    "- To overcom the issue mentioned above, we create a binary attribute for each category. This is also called one-hot encoding where only one of the binary attributes is hot and the others are cold\n",
    "- If we have a very large number of categories, this will create a very large table with zeroes and this will need a lot of memory. But to overcome this python creates a Scipy sparse matrix which only stores the location of the non-zero element.\n",
    "- If really required, the spare matrix can be converted to a dense numpy matrix\n",
    "- When the number of categories becomes too large, then it is best to use embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Building a custom class to do custom data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Create a class and implement 3 methods fit, transform and fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Feature Scaling - very important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most of the ML algorithms do not perform well when the numerical attributes are of different scale.\n",
    "** Note that it is generally not required to scale the target variable **\n",
    "Two common ways of applying feature scaling, are:\n",
    "1. min-max scaling also called normalization\n",
    "    - **Pros**\n",
    "        -  Brings the data range between 0-1 which is more favorable, especially to neural networks\n",
    "    - **Cons**\n",
    "        -  Very sensitive to outliers since it uses min and max values of an attribute\n",
    "2. standardization\n",
    "    - **Pros**\n",
    "        -  Less susecptible to outliers\n",
    "    \n",
    "    - **Cons**\n",
    "         -  Does not bring down the data range between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a pipeline that will take the dataset as an input and give the output as a transformed data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run into 2 problems when fitting a model to the data, they are:\n",
    "\n",
    "Underfitting:\n",
    "    1. this is oversimplying the model\n",
    "    2. this is caused because of error due to bias\n",
    "    3. This model performs bad on both the training and testing set\n",
    "    \n",
    "Overfitting:\n",
    "    1. this is caused because of overcomplicating the problem\n",
    "    2. this is cause because of error due to variance\n",
    "    3. This model performes well on the training set but bad on the test set\n",
    "    \n",
    "## CROSS VALIDATION    \n",
    "In order to detect the problems of underfitting and overfitting, we use the model complexity graph which shows the error our model makes on the training data and testing data, but we should not use the testing data to tune our model. To overcome this we make use of a validation set.\n",
    "\n",
    "The test dataset is kept aside. The training dataset is now further split into training set and validation set in order to evaluate our model. This is also called as ** CROSS VALIDATION **\n",
    "It is important to shuffle the dataset while doing cross validation to remove any hint of bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model complexity graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model complexity curve is used to understand if our model is over fitting or underfitting.\n",
    "Procedure of building a learning curve\n",
    "1. For an estimator/model type, say using Linear Regression\n",
    "2. Take all training points and build a model M11 with order 1\n",
    "3. Now build another model by incresing its complexity say order=2\n",
    "4. Build a model complexity curve graph using the method of cross validation, where on the y-axis plot the error of the models and on the x-axis plot the complexity used e.g order of the variables used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='model_complexity_graph.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve is used to understand if our model is over fitting or underfitting.\n",
    "Procedure of building a learning curve\n",
    "1. For an estimator/model type, say using Linear Regression\n",
    "2. Take few training points and build a model M11\n",
    "3. Now, take a few more training points and add them to the previous set and build model M12\n",
    "4. Build a model learning curve graph using the method of cross validation, where on the y-axis plot the error of the models and on the x-axis plot the number of training points used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img scr='learning_curves.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='learning_curves.JPG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
